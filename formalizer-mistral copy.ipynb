{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "99191346",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/Hearlvein/formalizer/blob/main/formalize-mistral.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b07c1652",
   "metadata": {},
   "source": [
    "# 🎯 Fine-Tuning Mistral-7B for Formality Translation with Instruction Prompting\n",
    "\n",
    "This notebook fine-tunes Mistral-7B-Instruct-v0.3 for translating informal text to formal text. Key optimizations for T4 GPU (15GB VRAM):\n",
    "- 4-bit quantization with QLoRA for memory efficiency\n",
    "- Instruction-based prompting compatible with Mistral's chat format\n",
    "- Optimized batch sizes and gradient accumulation\n",
    "- Dataset formatted for instruction-following tasks\n",
    "- **CPU fallback support** for systems without CUDA\n",
    "\n",
    "**Task:** Transform informal text into professional, formal equivalents while preserving meaning.\n",
    "\n",
    "**Hardware Requirements:** T4 GPU with 15GB VRAM (Google Colab compatible) or CPU (slower training)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0c2ad4d",
   "metadata": {},
   "source": [
    "## 🔧 Setup and Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd6e7b66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping Flash Attention installation (CPU mode)\n",
      "CUDA available: False\n",
      "Running in CPU mode - training will be slower but functional\n",
      "Consider using a smaller dataset or reduced epochs for CPU training\n"
     ]
    }
   ],
   "source": [
    "# Install necessary packages with specific versions for T4 compatibility\n",
    "!pip install -q torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "!pip install -q transformers==4.36.0 datasets==2.14.0 peft==0.7.0 trl==0.7.4\n",
    "!pip install -q accelerate==0.24.0 bitsandbytes==0.41.3 optimum==1.14.0\n",
    "!pip install -q pandas scikit-learn nltk matplotlib seaborn\n",
    "# Only install flash-attn if CUDA is available\n",
    "try:\n",
    "    import torch\n",
    "    if torch.cuda.is_available():\n",
    "        !pip install -q flash-attn --no-build-isolation\n",
    "        print(\"Flash Attention installed for GPU acceleration\")\n",
    "    else:\n",
    "        print(\"Skipping Flash Attention installation (CPU mode)\")\n",
    "except Exception as e:\n",
    "    print(f\"Flash Attention installation failed: {e}\")\n",
    "\n",
    "# Check GPU and memory\n",
    "import torch\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name()}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "else:\n",
    "    print(\"Running in CPU mode - training will be slower but functional\")\n",
    "    print(\"Consider using a smaller dataset or reduced epochs for CPU training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1857c99d",
   "metadata": {},
   "source": [
    "## 📚 Dataset Preparation for Instruction Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "445f32d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded with 2000 pairs\n",
      "Sample data:\n",
      "                                              formal  \\\n",
      "0  We kindly ask that you the system update will ...   \n",
      "1  Good morning, I regret the oversight and will ...   \n",
      "2  We kindly ask that you we have identified a di...   \n",
      "3  Esteemed colleagues, I regret the oversight an...   \n",
      "4  I would appreciate it if you could we require ...   \n",
      "\n",
      "                                            informal  \n",
      "0  We'd like you to we'll update the system this ...  \n",
      "1  Morning! My bad, I'll fix it ASAP. Mind sendin...  \n",
      "2  We'd like you to we found a mistake in the dat...  \n",
      "3  Hey folks, My bad, I'll fix it ASAP. Let me kn...  \n",
      "4  I'd be grateful if you we need more info to mo...  \n",
      "After cleaning: 2000 pairs\n",
      "Length distribution:\n",
      "length_category\n",
      "medium    1294\n",
      "long       612\n",
      "short       94\n",
      "Name: count, dtype: int64\n",
      "Training set: 1600 pairs\n",
      "Validation set: 400 pairs\n",
      "\n",
      "Selected few-shot examples for Mistral:\n",
      "\n",
      "1. Informal: Just so you’re aware we'll do server maintenance at midnight. Any questions, just let me know.\n",
      "   Formal: It is important to highlight that the server maintenance is scheduled at midnight. Should you have any questions, please reach out.\n",
      "\n",
      "2. Informal: Hey there! My bad, I'll fix it ASAP. Can you take a look at the attached doc? Thanks for your help! Talk soon,\n",
      "   Formal: Good afternoon, I regret the oversight and will correct it promptly. I would appreciate your assistance in reviewing the attached document. Thank you for your cooperation. Best regards,\n",
      "\n",
      "3. Informal: Hey everyone, Sorry for the late reply. Can you take a look at the attached doc? Thanks for your help! Talk soon,\n",
      "   Formal: Dear Sir or Madam, Please accept my apologies for the delay in response. I would appreciate your assistance in reviewing the attached document. Thank you for your cooperation. Best regards,\n",
      "\n",
      "Selected few-shot examples for Mistral:\n",
      "\n",
      "1. Informal: Just so you’re aware we'll do server maintenance at midnight. Any questions, just let me know.\n",
      "   Formal: It is important to highlight that the server maintenance is scheduled at midnight. Should you have any questions, please reach out.\n",
      "\n",
      "2. Informal: Hey there! My bad, I'll fix it ASAP. Can you take a look at the attached doc? Thanks for your help! Talk soon,\n",
      "   Formal: Good afternoon, I regret the oversight and will correct it promptly. I would appreciate your assistance in reviewing the attached document. Thank you for your cooperation. Best regards,\n",
      "\n",
      "3. Informal: Hey everyone, Sorry for the late reply. Can you take a look at the attached doc? Thanks for your help! Talk soon,\n",
      "   Formal: Dear Sir or Madam, Please accept my apologies for the delay in response. I would appreciate your assistance in reviewing the attached document. Thank you for your cooperation. Best regards,\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import random\n",
    "from pathlib import Path\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.cluster import KMeans\n",
    "from typing import List, Tuple, Dict\n",
    "import re\n",
    "import os\n",
    "import datetime\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Load the valentin dataset\n",
    "dataset_path = \"valentin_dataset.csv\"\n",
    "df = pd.read_csv(dataset_path, sep=';')\n",
    "\n",
    "print(f\"Dataset loaded with {len(df)} pairs\")\n",
    "print(\"Sample data:\")\n",
    "print(df.head())\n",
    "\n",
    "# Clean and validate the data\n",
    "def clean_text(text):\n",
    "    \"\"\"Clean text by removing extra whitespace and normalizing\"\"\"\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    return re.sub(r'\\s+', ' ', str(text).strip())\n",
    "\n",
    "df['formal'] = df['formal'].apply(clean_text)\n",
    "df['informal'] = df['informal'].apply(clean_text)\n",
    "\n",
    "# Remove empty or very short entries\n",
    "df = df[(df['formal'].str.len() > 10) & (df['informal'].str.len() > 10)]\n",
    "print(f\"After cleaning: {len(df)} pairs\")\n",
    "\n",
    "# Create stratified split based on text length to ensure balanced distribution\n",
    "def get_text_length_category(text):\n",
    "    \"\"\"Categorize text by length for stratified splitting\"\"\"\n",
    "    length = len(text.split())\n",
    "    if length <= 10:\n",
    "        return 'short'\n",
    "    elif length <= 20:\n",
    "        return 'medium'\n",
    "    else:\n",
    "        return 'long'\n",
    "\n",
    "df['length_category'] = df['informal'].apply(get_text_length_category)\n",
    "print(\"Length distribution:\")\n",
    "print(df['length_category'].value_counts())\n",
    "\n",
    "# Stratified split to maintain length distribution\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_df, val_df = train_test_split(\n",
    "    df, \n",
    "    test_size=0.2, \n",
    "    random_state=42, \n",
    "    stratify=df['length_category']\n",
    ")\n",
    "\n",
    "print(f\"Training set: {len(train_df)} pairs\")\n",
    "print(f\"Validation set: {len(val_df)} pairs\")\n",
    "\n",
    "def select_diverse_examples_for_mistral(df: pd.DataFrame, n_examples: int = 3) -> List[Tuple[str, str]]:\n",
    "    \"\"\"\n",
    "    Select diverse examples optimized for Mistral's context window.\n",
    "    Using fewer examples (3) to leave more room for the actual conversation.\n",
    "    \"\"\"\n",
    "    # Use TF-IDF to convert text to vectors\n",
    "    vectorizer = TfidfVectorizer(max_features=500, stop_words='english')\n",
    "    informal_vectors = vectorizer.fit_transform(df['informal'])\n",
    "\n",
    "    # Apply K-means clustering\n",
    "    n_clusters = min(n_examples, len(df))\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\n",
    "    cluster_labels = kmeans.fit_predict(informal_vectors)\n",
    "\n",
    "    # Find examples closest to each cluster center\n",
    "    selected_examples = []\n",
    "    for i in range(n_clusters):\n",
    "        cluster_indices = np.where(cluster_labels == i)[0]\n",
    "        if len(cluster_indices) == 0:\n",
    "            continue\n",
    "\n",
    "        # Find the example closest to the cluster center\n",
    "        cluster_center = kmeans.cluster_centers_[i:i+1]\n",
    "        distances = []\n",
    "        for idx in cluster_indices:\n",
    "            dist = cosine_similarity(informal_vectors[idx:idx+1], cluster_center)[0][0]\n",
    "            distances.append((idx, dist))\n",
    "\n",
    "        # Get the closest example (highest similarity)\n",
    "        closest_idx = sorted(distances, key=lambda x: x[1], reverse=True)[0][0]\n",
    "        selected_examples.append((df.iloc[closest_idx]['informal'], df.iloc[closest_idx]['formal']))\n",
    "\n",
    "    return selected_examples\n",
    "\n",
    "# Select diverse examples for few-shot prompting (fewer for Mistral)\n",
    "few_shot_examples = select_diverse_examples_for_mistral(df, n_examples=3)\n",
    "\n",
    "print(\"\\nSelected few-shot examples for Mistral:\")\n",
    "for i, (informal, formal) in enumerate(few_shot_examples, 1):\n",
    "    print(f\"\\n{i}. Informal: {informal}\")\n",
    "    print(f\"   Formal: {formal}\")\n",
    "\n",
    "# Create experiment directory\n",
    "timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "experiment_dir = Path(f\"mistral_formality_model_{timestamp}\")\n",
    "experiment_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Save few-shot examples\n",
    "with open(experiment_dir / \"few_shot_examples.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump([(inf, form) for inf, form in few_shot_examples], f, ensure_ascii=False, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f4bc885",
   "metadata": {},
   "source": [
    "## 🎯 Mistral Instruction Format and Dataset Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c640543b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 1598 training examples\n",
      "Created 399 validation examples\n",
      "Training data saved to mistral_formality_model_20250616_233356\\mistral_train_dataset.jsonl\n",
      "Validation data saved to mistral_formality_model_20250616_233356\\mistral_val_dataset.jsonl\n",
      "\n",
      "Sample Mistral instruction format:\n",
      "<s>[INST] You are an expert writing assistant specializing in formality transformation. Your task is to convert informal text into formal, professional language suitable for business or academic contexts.\n",
      "\n",
      "Guidelines:\n",
      "- Preserve the original meaning completely\n",
      "- Use professional vocabulary and sentence structure\n",
      "- Maintain appropriate tone and register\n",
      "- Output only the formal version without explanations\n",
      "\n",
      "Here are some examples:\n",
      "\n",
      "Informal: Just so you’re aware we'll do server maintenance at midnight. Any questions, just let me know.\n",
      "Formal: It is important to highlight that the server maintenance is scheduled at midnight. Should you have any questions, please reach out.\n",
      "\n",
      "Informal: Hey there! My bad, I'll fix it ASAP. Can you take a look at the attached doc? Thanks for your help! Talk soon,\n",
      "Formal: Good afternoon, I regret the oversight and will correct it promptly. I would appreciate your assistance in reviewing the attached document. Thank you for your cooperation. Best regards,\n",
      "\n",
      "Informal: Hey everyone, Sorry for the late reply. Can you take a look at the attached doc? Thanks for your help! Talk soon,\n",
      "Formal: Dear Sir or Madam, Please accept my apologies for the delay in response. I would appreciate your assistance in reviewing the attached document. Thank you for your cooperation. Best regards,\n",
      "\n",
      "Now convert this informal text to formal:\n",
      "\n",
      "Informal: Hey, can you help me out?\n",
      "Formal: [/INST]\n",
      "Expected response: I would appreciate your assistance.\n"
     ]
    }
   ],
   "source": [
    "def create_mistral_instruction_prompt(examples: List[Tuple[str, str]], test_informal: str = None) -> str:\n",
    "    \"\"\"\n",
    "    Create instruction prompt in Mistral's chat format.\n",
    "    Mistral-7B-Instruct uses <s>[INST] instruction [/INST] response format.\n",
    "    \"\"\"\n",
    "    \n",
    "    # System instruction for the task\n",
    "    system_instruction = \"\"\"You are an expert writing assistant specializing in formality transformation. Your task is to convert informal text into formal, professional language suitable for business or academic contexts.\n",
    "\n",
    "Guidelines:\n",
    "- Preserve the original meaning completely\n",
    "- Use professional vocabulary and sentence structure\n",
    "- Maintain appropriate tone and register\n",
    "- Output only the formal version without explanations\n",
    "\n",
    "Here are some examples:\"\"\"\n",
    "\n",
    "    # Add few-shot examples to the instruction\n",
    "    examples_text = \"\"\n",
    "    for informal, formal in examples:\n",
    "        examples_text += f\"\\n\\nInformal: {informal}\\nFormal: {formal}\"\n",
    "    \n",
    "    if test_informal:\n",
    "        full_instruction = f\"{system_instruction}{examples_text}\\n\\nNow convert this informal text to formal:\\n\\nInformal: {test_informal}\\nFormal:\"\n",
    "        return f\"<s>[INST] {full_instruction} [/INST]\"\n",
    "    else:\n",
    "        return f\"{system_instruction}{examples_text}\"\n",
    "\n",
    "def create_mistral_training_dataset(train_df: pd.DataFrame, val_df: pd.DataFrame, few_shot_examples: List[Tuple[str, str]]) -> Tuple[List[dict], List[dict]]:\n",
    "    \"\"\"\n",
    "    Create training and validation datasets in Mistral's instruction format.\n",
    "    Each example is a complete conversation with instruction and response.\n",
    "    \"\"\"\n",
    "    training_data = []\n",
    "    validation_data = []\n",
    "    \n",
    "    # Create a set of few-shot examples to exclude\n",
    "    few_shot_informals = {informal for informal, _ in few_shot_examples}\n",
    "    \n",
    "    def create_conversation(informal_text: str, formal_text: str) -> dict:\n",
    "        \"\"\"Create a single training example in conversation format\"\"\"\n",
    "        instruction = create_mistral_instruction_prompt(few_shot_examples, informal_text)\n",
    "        response = formal_text\n",
    "        \n",
    "        # Complete conversation format for training\n",
    "        conversation = f\"{instruction} {response}</s>\"\n",
    "        \n",
    "        return {\n",
    "            \"text\": conversation,\n",
    "            \"instruction\": instruction,\n",
    "            \"response\": response,\n",
    "            \"informal\": informal_text,\n",
    "            \"formal\": formal_text\n",
    "        }\n",
    "    \n",
    "    # Process training data\n",
    "    for _, row in train_df.iterrows():\n",
    "        if row['informal'] not in few_shot_informals:\n",
    "            training_data.append(create_conversation(row['informal'], row['formal']))\n",
    "    \n",
    "    # Process validation data\n",
    "    for _, row in val_df.iterrows():\n",
    "        if row['informal'] not in few_shot_informals:\n",
    "            validation_data.append(create_conversation(row['informal'], row['formal']))\n",
    "    \n",
    "    return training_data, validation_data\n",
    "\n",
    "# Create training and validation datasets\n",
    "training_data, validation_data = create_mistral_training_dataset(train_df, val_df, few_shot_examples)\n",
    "print(f\"Created {len(training_data)} training examples\")\n",
    "print(f\"Created {len(validation_data)} validation examples\")\n",
    "\n",
    "# Save datasets\n",
    "train_file = experiment_dir / \"mistral_train_dataset.jsonl\"\n",
    "with train_file.open(\"w\", encoding=\"utf-8\") as f:\n",
    "    for item in training_data:\n",
    "        f.write(json.dumps(item, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "val_file = experiment_dir / \"mistral_val_dataset.jsonl\"\n",
    "with val_file.open(\"w\", encoding=\"utf-8\") as f:\n",
    "    for item in validation_data:\n",
    "        f.write(json.dumps(item, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "print(f\"Training data saved to {train_file}\")\n",
    "print(f\"Validation data saved to {val_file}\")\n",
    "\n",
    "# Show example conversation\n",
    "sample_instruction = create_mistral_instruction_prompt(few_shot_examples, \"Hey, can you help me out?\")\n",
    "print(\"\\nSample Mistral instruction format:\")\n",
    "print(sample_instruction)\n",
    "print(\"Expected response: I would appreciate your assistance.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bb5a9ec",
   "metadata": {},
   "source": [
    "## 🧠 Mistral-7B Fine-Tuning with QLoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0f15c63b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\james\\anaconda3\\envs\\formalizer-venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\james\\anaconda3\\envs\\formalizer-venv\\Lib\\site-packages\\transformers\\utils\\generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "c:\\Users\\james\\anaconda3\\envs\\formalizer-venv\\Lib\\site-packages\\transformers\\utils\\generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "W0616 23:33:59.522000 25364 site-packages\\torch\\distributed\\elastic\\multiprocessing\\redirects.py:29] NOTE: Redirects are currently not supported in Windows or MacOs.\n",
      "W0616 23:33:59.522000 25364 site-packages\\torch\\distributed\\elastic\\multiprocessing\\redirects.py:29] NOTE: Redirects are currently not supported in Windows or MacOs.\n",
      "c:\\Users\\james\\anaconda3\\envs\\formalizer-venv\\Lib\\site-packages\\transformers\\utils\\generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "c:\\Users\\james\\anaconda3\\envs\\formalizer-venv\\Lib\\site-packages\\transformers\\utils\\generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "c:\\Users\\james\\anaconda3\\envs\\formalizer-venv\\Lib\\site-packages\\transformers\\utils\\generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "c:\\Users\\james\\anaconda3\\envs\\formalizer-venv\\Lib\\site-packages\\bitsandbytes\\cextension.py:34: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
      "  warn(\"The installed version of bitsandbytes was compiled without GPU support. \"\n",
      "c:\\Users\\james\\anaconda3\\envs\\formalizer-venv\\Lib\\site-packages\\transformers\\utils\\generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "c:\\Users\\james\\anaconda3\\envs\\formalizer-venv\\Lib\\site-packages\\bitsandbytes\\cextension.py:34: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
      "  warn(\"The installed version of bitsandbytes was compiled without GPU support. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'NoneType' object has no attribute 'cadam32bit_grad_fp32'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\james\\anaconda3\\envs\\formalizer-venv\\Lib\\site-packages\\trl\\trainer\\ppo_config.py:141: UserWarning: The `optimize_cuda_cache` arguement will be deprecated soon, please use `optimize_device_cache` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "CPU mode: Reduced batch sizes and no quantization\n",
      "Loading mistralai/Mistral-7B-Instruct-v0.3 in full precision for CPU...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\james\\anaconda3\\envs\\formalizer-venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\james\\anaconda3\\envs\\formalizer-venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\james\\.cache\\huggingface\\hub\\models--mistralai--Mistral-7B-Instruct-v0.3. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "c:\\Users\\james\\anaconda3\\envs\\formalizer-venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\james\\.cache\\huggingface\\hub\\models--mistralai--Mistral-7B-Instruct-v0.3. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "data did not match any variant of untagged enum PyPreTokenizerTypeWrapper at line 6952 column 3",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mException\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 52\u001b[39m\n\u001b[32m     49\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mLoading \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mMODEL_NAME\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m in full precision for CPU...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     51\u001b[39m \u001b[38;5;66;03m# Load tokenizer\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m tokenizer = \u001b[43mAutoTokenizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mMODEL_NAME\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     53\u001b[39m tokenizer.pad_token = tokenizer.eos_token\n\u001b[32m     54\u001b[39m tokenizer.padding_side = \u001b[33m\"\u001b[39m\u001b[33mright\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\james\\anaconda3\\envs\\formalizer-venv\\Lib\\site-packages\\transformers\\models\\auto\\tokenization_auto.py:787\u001b[39m, in \u001b[36mAutoTokenizer.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[39m\n\u001b[32m    783\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m tokenizer_class \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    784\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    785\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTokenizer class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtokenizer_class_candidate\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m does not exist or is not currently imported.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    786\u001b[39m         )\n\u001b[32m--> \u001b[39m\u001b[32m787\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtokenizer_class\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    789\u001b[39m \u001b[38;5;66;03m# Otherwise we have to be creative.\u001b[39;00m\n\u001b[32m    790\u001b[39m \u001b[38;5;66;03m# if model is an encoder decoder, the encoder tokenizer class is used by default\u001b[39;00m\n\u001b[32m    791\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(config, EncoderDecoderConfig):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\james\\anaconda3\\envs\\formalizer-venv\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2028\u001b[39m, in \u001b[36mPreTrainedTokenizerBase.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, *init_inputs, **kwargs)\u001b[39m\n\u001b[32m   2025\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   2026\u001b[39m         logger.info(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mloading file \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m from cache at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresolved_vocab_files[file_id]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m2028\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_from_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2029\u001b[39m \u001b[43m    \u001b[49m\u001b[43mresolved_vocab_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2030\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2031\u001b[39m \u001b[43m    \u001b[49m\u001b[43minit_configuration\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2032\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43minit_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2033\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2034\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2035\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2036\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2037\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_is_local\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_local\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2038\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2039\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\james\\anaconda3\\envs\\formalizer-venv\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2260\u001b[39m, in \u001b[36mPreTrainedTokenizerBase._from_pretrained\u001b[39m\u001b[34m(cls, resolved_vocab_files, pretrained_model_name_or_path, init_configuration, token, cache_dir, local_files_only, _commit_hash, _is_local, *init_inputs, **kwargs)\u001b[39m\n\u001b[32m   2258\u001b[39m \u001b[38;5;66;03m# Instantiate the tokenizer.\u001b[39;00m\n\u001b[32m   2259\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2260\u001b[39m     tokenizer = \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43minit_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43minit_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2261\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m:\n\u001b[32m   2262\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\n\u001b[32m   2263\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mUnable to load vocabulary from file. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2264\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mPlease check that the provided vocabulary is accessible and not corrupted.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2265\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\james\\anaconda3\\envs\\formalizer-venv\\Lib\\site-packages\\transformers\\models\\llama\\tokenization_llama_fast.py:124\u001b[39m, in \u001b[36mLlamaTokenizerFast.__init__\u001b[39m\u001b[34m(self, vocab_file, tokenizer_file, clean_up_tokenization_spaces, unk_token, bos_token, eos_token, add_bos_token, add_eos_token, use_default_system_prompt, **kwargs)\u001b[39m\n\u001b[32m    111\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\n\u001b[32m    112\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    113\u001b[39m     vocab_file=\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    122\u001b[39m     **kwargs,\n\u001b[32m    123\u001b[39m ):\n\u001b[32m--> \u001b[39m\u001b[32m124\u001b[39m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    125\u001b[39m \u001b[43m        \u001b[49m\u001b[43mvocab_file\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvocab_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    126\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtokenizer_file\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtokenizer_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    127\u001b[39m \u001b[43m        \u001b[49m\u001b[43mclean_up_tokenization_spaces\u001b[49m\u001b[43m=\u001b[49m\u001b[43mclean_up_tokenization_spaces\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    128\u001b[39m \u001b[43m        \u001b[49m\u001b[43munk_token\u001b[49m\u001b[43m=\u001b[49m\u001b[43munk_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    129\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbos_token\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbos_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    130\u001b[39m \u001b[43m        \u001b[49m\u001b[43meos_token\u001b[49m\u001b[43m=\u001b[49m\u001b[43meos_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    131\u001b[39m \u001b[43m        \u001b[49m\u001b[43madd_bos_token\u001b[49m\u001b[43m=\u001b[49m\u001b[43madd_bos_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    132\u001b[39m \u001b[43m        \u001b[49m\u001b[43madd_eos_token\u001b[49m\u001b[43m=\u001b[49m\u001b[43madd_eos_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    133\u001b[39m \u001b[43m        \u001b[49m\u001b[43muse_default_system_prompt\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_default_system_prompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    134\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    135\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    136\u001b[39m     \u001b[38;5;28mself\u001b[39m._add_bos_token = add_bos_token\n\u001b[32m    137\u001b[39m     \u001b[38;5;28mself\u001b[39m._add_eos_token = add_eos_token\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\james\\anaconda3\\envs\\formalizer-venv\\Lib\\site-packages\\transformers\\tokenization_utils_fast.py:111\u001b[39m, in \u001b[36mPreTrainedTokenizerFast.__init__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    108\u001b[39m     fast_tokenizer = copy.deepcopy(tokenizer_object)\n\u001b[32m    109\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m fast_tokenizer_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m from_slow:\n\u001b[32m    110\u001b[39m     \u001b[38;5;66;03m# We have a serialization from tokenizers which let us directly build the backend\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m111\u001b[39m     fast_tokenizer = \u001b[43mTokenizerFast\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfast_tokenizer_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    112\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m slow_tokenizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    113\u001b[39m     \u001b[38;5;66;03m# We need to convert a slow tokenizer to build the backend\u001b[39;00m\n\u001b[32m    114\u001b[39m     fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)\n",
      "\u001b[31mException\u001b[39m: data did not match any variant of untagged enum PyPreTokenizerTypeWrapper at line 6952 column 3"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    pipeline\n",
    ")\n",
    "from peft import get_peft_model, LoraConfig, prepare_model_for_kbit_training, TaskType\n",
    "from trl import SFTTrainer, DataCollatorForCompletionOnlyLM\n",
    "import gc\n",
    "\n",
    "# Model configuration for T4 GPU optimization\n",
    "MODEL_NAME = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "base_output_dir = \"./mistral_formality_model\"\n",
    "\n",
    "# Check device availability\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "use_gpu = torch.cuda.is_available()\n",
    "\n",
    "print(f\"Using device: {device}\")\n",
    "if use_gpu:\n",
    "    print(\"GPU mode: Optimized for T4 GPU with quantization\")\n",
    "else:\n",
    "    print(\"CPU mode: Reduced batch sizes and no quantization\")\n",
    "\n",
    "# Memory optimization settings for T4 GPU\n",
    "def cleanup_memory():\n",
    "    \"\"\"Clean up GPU memory\"\"\"\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "cleanup_memory()\n",
    "\n",
    "# Configure quantization only for GPU\n",
    "if use_gpu:\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_quant_storage=torch.bfloat16\n",
    "    )\n",
    "    print(f\"Loading {MODEL_NAME} with 4-bit quantization...\")\n",
    "else:\n",
    "    bnb_config = None\n",
    "    print(f\"Loading {MODEL_NAME} in full precision for CPU...\")\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "# Load model with device-appropriate configuration\n",
    "if use_gpu:\n",
    "    # GPU configuration with quantization\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        quantization_config=bnb_config,\n",
    "        device_map=\"auto\",\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        attn_implementation=\"flash_attention_2\",  # Use flash attention for efficiency\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    # Prepare model for k-bit training\n",
    "    model = prepare_model_for_kbit_training(model)\n",
    "    print(f\"Model loaded on GPU. Memory allocated: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")\n",
    "else:\n",
    "    # CPU configuration without quantization\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        torch_dtype=torch.float32,  # Use float32 for CPU\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    model = model.to(device)\n",
    "    print(\"Model loaded on CPU\")\n",
    "\n",
    "# Configure LoRA with device-appropriate parameters\n",
    "if use_gpu:\n",
    "    # Higher rank for GPU with quantization\n",
    "    lora_config = LoraConfig(\n",
    "        r=64,  # Higher rank for better performance on larger model\n",
    "        lora_alpha=128,  # 2x the rank\n",
    "        target_modules=[\n",
    "            \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "            \"gate_proj\", \"up_proj\", \"down_proj\"\n",
    "        ],\n",
    "        lora_dropout=0.1,\n",
    "        bias=\"none\",\n",
    "        task_type=TaskType.CAUSAL_LM,\n",
    "        inference_mode=False\n",
    "    )\n",
    "else:\n",
    "    # Lower rank for CPU to reduce memory usage\n",
    "    lora_config = LoraConfig(\n",
    "        r=16,  # Reduced rank for CPU\n",
    "        lora_alpha=32,  # 2x the rank\n",
    "        target_modules=[\n",
    "            \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "            \"gate_proj\", \"up_proj\", \"down_proj\"\n",
    "        ],\n",
    "        lora_dropout=0.1,\n",
    "        bias=\"none\",\n",
    "        task_type=TaskType.CAUSAL_LM,\n",
    "        inference_mode=False\n",
    "    )\n",
    "\n",
    "# Apply LoRA\n",
    "model = get_peft_model(model, lora_config)\n",
    "print(f\"LoRA applied. Trainable parameters: {model.num_parameters(only_trainable=True):,}\")\n",
    "print(f\"Total parameters: {model.num_parameters():,}\")\n",
    "print(f\"Trainable %: {100 * model.num_parameters(only_trainable=True) / model.num_parameters():.2f}%\")\n",
    "\n",
    "# Load datasets\n",
    "def load_jsonl_dataset(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        data = [json.loads(line) for line in f if line.strip()]\n",
    "    return Dataset.from_list(data)\n",
    "\n",
    "train_dataset = load_jsonl_dataset(train_file)\n",
    "val_dataset = load_jsonl_dataset(val_file)\n",
    "\n",
    "print(f\"Loaded {len(train_dataset)} training examples\")\n",
    "print(f\"Loaded {len(val_dataset)} validation examples\")\n",
    "\n",
    "# Calculate optimal batch size based on device\n",
    "if use_gpu:\n",
    "    # GPU configuration - optimized for T4 GPU (15GB VRAM)\n",
    "    per_device_batch_size = 2\n",
    "    gradient_accumulation_steps = 8  # Effective batch size = 2 * 8 = 16\n",
    "    fp16_bf16 = True\n",
    "    dataloader_pin_memory = False\n",
    "    gradient_checkpointing = True\n",
    "    optim = \"paged_adamw_8bit\"  # 8-bit optimizer for memory efficiency\n",
    "else:\n",
    "    # CPU configuration - reduced batch sizes\n",
    "    per_device_batch_size = 1\n",
    "    gradient_accumulation_steps = 4  # Effective batch size = 1 * 4 = 4\n",
    "    fp16_bf16 = False  # No mixed precision on CPU\n",
    "    dataloader_pin_memory = True  # Can pin memory on CPU\n",
    "    gradient_checkpointing = False  # Disable to save CPU compute\n",
    "    optim = \"adamw_torch\"  # Standard optimizer for CPU\n",
    "\n",
    "# Optimized training arguments with device-specific settings\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=base_output_dir,\n",
    "    per_device_train_batch_size=per_device_batch_size,\n",
    "    per_device_eval_batch_size=per_device_batch_size,\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "    learning_rate=2e-4 if use_gpu else 1e-4,  # Lower LR for CPU\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_ratio=0.03,\n",
    "    num_train_epochs=3 if use_gpu else 1,  # Fewer epochs for CPU\n",
    "    max_steps=-1,\n",
    "    logging_steps=10,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=50 if use_gpu else 100,  # Less frequent eval on CPU\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=50 if use_gpu else 100,\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    "    bf16=fp16_bf16 if use_gpu else False,\n",
    "    fp16=False,  # Use bf16 instead of fp16 for GPU\n",
    "    dataloader_pin_memory=dataloader_pin_memory,\n",
    "    gradient_checkpointing=gradient_checkpointing,\n",
    "    optim=optim,\n",
    "    weight_decay=0.01,\n",
    "    max_grad_norm=1.0,\n",
    "    seed=42,\n",
    "    report_to=\"none\",\n",
    "    remove_unused_columns=False\n",
    ")\n",
    "\n",
    "# Create data collator for completion-only training\n",
    "# This ensures we only train on the response part, not the instruction\n",
    "response_template = \"[/INST]\"\n",
    "data_collator = DataCollatorForCompletionOnlyLM(\n",
    "    response_template=response_template,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "# Initialize SFTTrainer with device-appropriate settings\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=1024 if use_gpu else 512,  # Shorter sequences for CPU\n",
    "    packing=False,  # Don't pack multiple examples together\n",
    ")\n",
    "\n",
    "print(f\"Starting Mistral formality translation training on {device.upper()}...\")\n",
    "if use_gpu:\n",
    "    print(f\"Memory before training: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")\n",
    "else:\n",
    "    print(\"CPU training will be slower but uses less memory\")\n",
    "\n",
    "# Train the model\n",
    "train_result = trainer.train()\n",
    "\n",
    "print(f\"Training complete. Final metrics: {train_result.metrics}\")\n",
    "if use_gpu:\n",
    "    print(f\"Memory after training: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")\n",
    "\n",
    "# Save the model\n",
    "model_path = os.path.join(base_output_dir, \"best_model\")\n",
    "trainer.save_model(model_path)\n",
    "tokenizer.save_pretrained(model_path)\n",
    "\n",
    "# Save training configuration\n",
    "config_data = {\n",
    "    \"model_name\": MODEL_NAME,\n",
    "    \"device\": device,\n",
    "    \"use_quantization\": use_gpu,\n",
    "    \"lora_config\": {\n",
    "        \"r\": lora_config.r,\n",
    "        \"alpha\": lora_config.lora_alpha,\n",
    "        \"target_modules\": lora_config.target_modules,\n",
    "        \"dropout\": lora_config.lora_dropout\n",
    "    },\n",
    "    \"training_args\": training_args.to_dict(),\n",
    "    \"dataset_size\": {\n",
    "        \"train\": len(train_dataset),\n",
    "        \"validation\": len(val_dataset)\n",
    "    },\n",
    "    \"few_shot_examples\": few_shot_examples\n",
    "}\n",
    "\n",
    "with open(os.path.join(base_output_dir, \"training_config.json\"), \"w\") as f:\n",
    "    json.dump(config_data, f, indent=2, default=str)\n",
    "\n",
    "print(f\"Mistral formality model saved to {model_path}\")\n",
    "cleanup_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21b14739",
   "metadata": {},
   "source": [
    "## ✨ Mistral Formality Translation Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f09640a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "from peft import PeftModel\n",
    "import re\n",
    "from functools import lru_cache\n",
    "\n",
    "# Load the fine-tuned Mistral model\n",
    "model_path = os.path.join(base_output_dir, \"best_model\")\n",
    "print(f\"Loading fine-tuned Mistral model from {model_path}\")\n",
    "\n",
    "cleanup_memory()\n",
    "\n",
    "# Check device availability for inference\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "use_gpu = torch.cuda.is_available()\n",
    "\n",
    "if use_gpu:\n",
    "    # GPU inference configuration\n",
    "    base_model = AutoModelForCausalLM.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        quantization_config=bnb_config,\n",
    "        device_map=\"auto\",\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    print(\"Loading model on GPU with quantization\")\n",
    "else:\n",
    "    # CPU inference configuration\n",
    "    base_model = AutoModelForCausalLM.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        torch_dtype=torch.float32,\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    base_model = base_model.to(device)\n",
    "    print(\"Loading model on CPU\")\n",
    "\n",
    "# Load LoRA adapter\n",
    "model = PeftModel.from_pretrained(base_model, model_path)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "print(\"Model loaded successfully!\")\n",
    "\n",
    "# Create generation pipeline with device-appropriate parameters\n",
    "if use_gpu:\n",
    "    generator = pipeline(\n",
    "        \"text-generation\",\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        device_map=\"auto\"\n",
    "    )\n",
    "else:\n",
    "    generator = pipeline(\n",
    "        \"text-generation\",\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        torch_dtype=torch.float32,\n",
    "        device=device\n",
    "    )\n",
    "\n",
    "@lru_cache(maxsize=64)\n",
    "def translate_to_formal_mistral(informal_text: str, examples_tuple: tuple) -> str:\n",
    "    \"\"\"\n",
    "    Translate informal text to formal using fine-tuned Mistral model.\n",
    "    \n",
    "    Args:\n",
    "        informal_text: The informal text to translate\n",
    "        examples_tuple: Tuple of few-shot examples for consistency\n",
    "    \n",
    "    Returns:\n",
    "        The formal translation\n",
    "    \"\"\"\n",
    "    # Convert tuple back to list\n",
    "    examples = list(examples_tuple)\n",
    "    \n",
    "    # Create instruction prompt\n",
    "    prompt = create_mistral_instruction_prompt(examples, informal_text)\n",
    "    \n",
    "    # Device-appropriate generation parameters\n",
    "    if use_gpu:\n",
    "        # GPU generation parameters\n",
    "        max_new_tokens = 150\n",
    "        temperature = 0.3\n",
    "        top_k = 40\n",
    "        top_p = 0.9\n",
    "    else:\n",
    "        # CPU generation parameters (more conservative)\n",
    "        max_new_tokens = 100\n",
    "        temperature = 0.2\n",
    "        top_k = 20\n",
    "        top_p = 0.8\n",
    "    \n",
    "    # Generate with optimized parameters for Mistral\n",
    "    output = generator(\n",
    "        prompt,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        do_sample=True,\n",
    "        temperature=temperature,\n",
    "        top_k=top_k,\n",
    "        top_p=top_p,\n",
    "        repetition_penalty=1.1,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        return_full_text=False  # Only return the generated part\n",
    "    )\n",
    "    \n",
    "    generated_text = output[0][\"generated_text\"].strip()\n",
    "    \n",
    "    # Post-process the output\n",
    "    # Remove any remaining instruction tokens or artifacts\n",
    "    formal_text = generated_text.replace(\"</s>\", \"\").strip()\n",
    "    \n",
    "    # Extract only the formal response (everything before potential continuation)\n",
    "    lines = formal_text.split('\\n')\n",
    "    formal_response = lines[0].strip()\n",
    "    \n",
    "    # Clean up any remaining artifacts\n",
    "    formal_response = re.sub(r'^(Formal:\\s*)?', '', formal_response)\n",
    "    formal_response = re.sub(r'\\s+', ' ', formal_response).strip()\n",
    "    \n",
    "    return formal_response\n",
    "\n",
    "# Test with validation examples\n",
    "test_examples = val_df.sample(5, random_state=42)\n",
    "few_shot_examples_tuple = tuple((inf, form) for inf, form in few_shot_examples)\n",
    "\n",
    "print(f\"🎯 Mistral Formality Translation Results ({device.upper()}):\\n\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for idx, row in test_examples.iterrows():\n",
    "    informal_input = row['informal']\n",
    "    expected_formal = row['formal']\n",
    "    \n",
    "    # Generate translation\n",
    "    predicted_formal = translate_to_formal_mistral(informal_input, few_shot_examples_tuple)\n",
    "    \n",
    "    print(f\"\\nInput (Informal): {informal_input}\")\n",
    "    print(f\"Expected (Formal): {expected_formal}\")\n",
    "    print(f\"Mistral Generated: {predicted_formal}\")\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "# Interactive testing function\n",
    "def interactive_mistral_test():\n",
    "    \"\"\"Interactive function to test Mistral formality translation\"\"\"\n",
    "    print(f\"\\n🔄 Interactive Mistral Formality Translation Test ({device.upper()})\")\n",
    "    print(\"Enter informal sentences to see their formal translations.\")\n",
    "    print(\"Type 'quit' to exit.\\n\")\n",
    "    \n",
    "    while True:\n",
    "        user_input = input(\"Informal sentence: \").strip()\n",
    "        \n",
    "        if user_input.lower() in ['quit', 'exit', 'q']:\n",
    "            break\n",
    "            \n",
    "        if not user_input:\n",
    "            continue\n",
    "        \n",
    "        formal_output = translate_to_formal_mistral(user_input, few_shot_examples_tuple)\n",
    "        print(f\"Formal translation: {formal_output}\\n\")\n",
    "\n",
    "# Test with various examples\n",
    "test_sentences = [\n",
    "    \"Hey, what's up?\",\n",
    "    \"Can you help me out with this thing?\",\n",
    "    \"Thanks a bunch for your help!\",\n",
    "    \"I'll get back to you ASAP.\",\n",
    "    \"Let me know if you need anything.\",\n",
    "    \"This is super important stuff.\",\n",
    "    \"We gotta finish this by tomorrow.\",\n",
    "    \"Sorry for the delay, my bad!\"\n",
    "]\n",
    "\n",
    "print(f\"\\n📝 Example Translations with Mistral ({device.upper()}):\")\n",
    "print(\"=\"*50)\n",
    "for informal in test_sentences:\n",
    "    formal = translate_to_formal_mistral(informal, few_shot_examples_tuple)\n",
    "    print(f\"• {informal}\")\n",
    "    print(f\"  → {formal}\\n\")\n",
    "\n",
    "# Performance note for CPU users\n",
    "if not use_gpu:\n",
    "    print(\"\\n💡 Note: Running on CPU. Inference is slower but functional.\")\n",
    "    print(\"For faster performance, consider using a GPU-enabled environment.\")\n",
    "\n",
    "# Uncomment to run interactive test\n",
    "# interactive_mistral_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3ae6125",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model for download\n",
    "import shutil\n",
    "from google.colab import files\n",
    "\n",
    "# Create a compressed archive of the model\n",
    "shutil.make_archive(\"mistral_formality_model\", 'zip', base_output_dir)\n",
    "files.download(\"mistral_formality_model.zip\")\n",
    "\n",
    "print(\"Model saved and ready for download!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52618ad3",
   "metadata": {},
   "source": [
    "## 📊 Mistral Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9da88f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "import nltk\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "# Download required NLTK data\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "except LookupError:\n",
    "    nltk.download('punkt')\n",
    "\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt_tab')\n",
    "except LookupError:\n",
    "    nltk.download('punkt_tab')\n",
    "\n",
    "# Enhanced evaluation function for Mistral\n",
    "def evaluate_mistral_formality(test_size: int = 25):\n",
    "    \"\"\"\n",
    "    Evaluate the Mistral formality translation model using multiple metrics.\n",
    "    Adjusted for CPU/GPU performance differences.\n",
    "    \n",
    "    Args:\n",
    "        test_size: Number of examples to test\n",
    "    \"\"\"\n",
    "    # Adjust test size based on device capability\n",
    "    if not use_gpu:\n",
    "        test_size = min(test_size, 10)  # Smaller test size for CPU\n",
    "        print(f\"Running smaller evaluation ({test_size} examples) on CPU for faster completion\")\n",
    "    \n",
    "    # Select test examples from validation set\n",
    "    test_df = val_df.sample(min(test_size, len(val_df)), random_state=42)\n",
    "    \n",
    "    predictions = []\n",
    "    references = []\n",
    "    informal_inputs = []\n",
    "    \n",
    "    print(f\"Evaluating Mistral formality translation on {len(test_df)} examples using {device.upper()}...\")\n",
    "    \n",
    "    # Metrics collection\n",
    "    bleu_scores = []\n",
    "    length_ratios = []\n",
    "    formality_improvements = []\n",
    "    \n",
    "    # Formality indicators (simplified for demonstration)\n",
    "    formal_words = [\n",
    "        'please', 'kindly', 'would', 'could', 'sincerely', 'respectfully',\n",
    "        'appreciate', 'grateful', 'thank you', 'regards', 'however',\n",
    "        'therefore', 'furthermore', 'consequently', 'nevertheless'\n",
    "    ]\n",
    "    \n",
    "    informal_words = [\n",
    "        'hey', 'hi', 'gonna', 'wanna', 'yeah', 'ok', 'cool', 'asap',\n",
    "        'btw', 'lol', 'omg', 'fyi', 'thanks', 'stuff', 'thing', 'super'\n",
    "    ]\n",
    "    \n",
    "    for idx, row in test_df.iterrows():\n",
    "        informal_input = row['informal']\n",
    "        expected_formal = row['formal']\n",
    "        \n",
    "        # Generate prediction\n",
    "        predicted_formal = translate_to_formal_mistral(informal_input, few_shot_examples_tuple)\n",
    "        \n",
    "        # Store results\n",
    "        informal_inputs.append(informal_input)\n",
    "        references.append(expected_formal)\n",
    "        predictions.append(predicted_formal)\n",
    "        \n",
    "        # Calculate BLEU score\n",
    "        smoothie = SmoothingFunction().method4\n",
    "        pred_tokens = nltk.word_tokenize(predicted_formal.lower())\n",
    "        ref_tokens = nltk.word_tokenize(expected_formal.lower())\n",
    "        bleu = sentence_bleu([ref_tokens], pred_tokens, smoothing_function=smoothie)\n",
    "        bleu_scores.append(bleu)\n",
    "        \n",
    "        # Calculate length ratio\n",
    "        length_ratio = len(predicted_formal) / len(informal_input) if len(informal_input) > 0 else 1\n",
    "        length_ratios.append(length_ratio)\n",
    "        \n",
    "        # Calculate formality improvement\n",
    "        def count_formality_words(text, word_list):\n",
    "            text_lower = text.lower()\n",
    "            return sum(1 for word in word_list if re.search(rf'\\b{re.escape(word)}\\b', text_lower))\n",
    "        \n",
    "        formal_count_pred = count_formality_words(predicted_formal, formal_words)\n",
    "        formal_count_input = count_formality_words(informal_input, formal_words)\n",
    "        informal_count_pred = count_formality_words(predicted_formal, informal_words)\n",
    "        informal_count_input = count_formality_words(informal_input, informal_words)\n",
    "        \n",
    "        # Formality improvement score\n",
    "        formality_gain = formal_count_pred - formal_count_input\n",
    "        informality_reduction = informal_count_input - informal_count_pred\n",
    "        formality_score = formality_gain + informality_reduction\n",
    "        formality_improvements.append(formality_score)\n",
    "        \n",
    "        # Progress indicator for CPU\n",
    "        if not use_gpu and (idx + 1) % 5 == 0:\n",
    "            print(f\"Processed {idx + 1}/{len(test_df)} examples...\")\n",
    "    \n",
    "    # Calculate summary metrics\n",
    "    avg_bleu = np.mean(bleu_scores)\n",
    "    avg_length_ratio = np.mean(length_ratios)\n",
    "    avg_formality_improvement = np.mean(formality_improvements)\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"\\n📊 Mistral Evaluation Results on {device.upper()} (n={len(test_df)}):\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Average BLEU Score: {avg_bleu:.3f}\")\n",
    "    print(f\"Average Length Ratio: {avg_length_ratio:.2f}\")\n",
    "    print(f\"Average Formality Improvement: {avg_formality_improvement:.2f}\")\n",
    "    \n",
    "    # Show sample results\n",
    "    print(f\"\\n📝 Sample Results:\")\n",
    "    for i in range(min(3, len(predictions))):\n",
    "        print(f\"\\nExample {i+1}:\")\n",
    "        print(f\"Informal: {informal_inputs[i]}\")\n",
    "        print(f\"Reference: {references[i]}\")\n",
    "        print(f\"Mistral: {predictions[i]}\")\n",
    "        print(f\"BLEU: {bleu_scores[i]:.3f}, Formality: {formality_improvements[i]:.1f}\")\n",
    "    \n",
    "    # Create visualizations\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    plt.suptitle(f'Mistral Formality Translation Evaluation ({device.upper()})', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # BLEU score distribution\n",
    "    plt.subplot(2, 3, 1)\n",
    "    plt.hist(bleu_scores, bins=10, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "    plt.axvline(avg_bleu, color='red', linestyle='--', linewidth=2)\n",
    "    plt.title(f'BLEU Scores (avg={avg_bleu:.3f})')\n",
    "    plt.xlabel('BLEU Score')\n",
    "    plt.ylabel('Count')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Length ratio distribution\n",
    "    plt.subplot(2, 3, 2)\n",
    "    plt.hist(length_ratios, bins=10, alpha=0.7, color='lightgreen', edgecolor='black')\n",
    "    plt.axvline(avg_length_ratio, color='red', linestyle='--', linewidth=2)\n",
    "    plt.title(f'Length Ratios (avg={avg_length_ratio:.2f})')\n",
    "    plt.xlabel('Output/Input Length Ratio')\n",
    "    plt.ylabel('Count')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Formality improvement distribution\n",
    "    plt.subplot(2, 3, 3)\n",
    "    plt.hist(formality_improvements, bins=10, alpha=0.7, color='orange', edgecolor='black')\n",
    "    plt.axvline(avg_formality_improvement, color='red', linestyle='--', linewidth=2)\n",
    "    plt.title(f'Formality Improvement (avg={avg_formality_improvement:.2f})')\n",
    "    plt.xlabel('Formality Score Change')\n",
    "    plt.ylabel('Count')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # BLEU vs Formality scatter plot\n",
    "    plt.subplot(2, 3, 4)\n",
    "    plt.scatter(bleu_scores, formality_improvements, alpha=0.6, color='purple')\n",
    "    plt.xlabel('BLEU Score')\n",
    "    plt.ylabel('Formality Improvement')\n",
    "    plt.title('BLEU vs Formality')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Box plots for metrics comparison\n",
    "    plt.subplot(2, 3, 5)\n",
    "    metrics_data = [bleu_scores, length_ratios, formality_improvements]\n",
    "    metrics_labels = ['BLEU', 'Length Ratio', 'Formality']\n",
    "    plt.boxplot(metrics_data, labels=metrics_labels)\n",
    "    plt.title('Metrics Distribution')\n",
    "    plt.ylabel('Score')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Example lengths comparison\n",
    "    plt.subplot(2, 3, 6)\n",
    "    input_lengths = [len(text.split()) for text in informal_inputs]\n",
    "    output_lengths = [len(text.split()) for text in predictions]\n",
    "    plt.scatter(input_lengths, output_lengths, alpha=0.6, color='brown')\n",
    "    plt.plot([0, max(input_lengths)], [0, max(input_lengths)], 'r--', alpha=0.8)\n",
    "    plt.xlabel('Input Length (words)')\n",
    "    plt.ylabel('Output Length (words)')\n",
    "    plt.title('Input vs Output Length')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(base_output_dir, 'mistral_evaluation_metrics.png'), dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    # Save detailed results\n",
    "    eval_results = {\n",
    "        'timestamp': datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "        'model': 'Mistral-7B-Instruct-v0.3',\n",
    "        'device': device,\n",
    "        'test_size': len(test_df),\n",
    "        'metrics': {\n",
    "            'bleu': {\n",
    "                'scores': bleu_scores,\n",
    "                'average': avg_bleu,\n",
    "                'std': np.std(bleu_scores)\n",
    "            },\n",
    "            'length_ratio': {\n",
    "                'ratios': length_ratios,\n",
    "                'average': avg_length_ratio,\n",
    "                'std': np.std(length_ratios)\n",
    "            },\n",
    "            'formality_improvement': {\n",
    "                'scores': formality_improvements,\n",
    "                'average': avg_formality_improvement,\n",
    "                'std': np.std(formality_improvements)\n",
    "            }\n",
    "        },\n",
    "        'examples': [\n",
    "            {\n",
    "                'informal': inf,\n",
    "                'reference': ref,\n",
    "                'prediction': pred,\n",
    "                'bleu': bleu,\n",
    "                'formality_score': form_score\n",
    "            }\n",
    "            for inf, ref, pred, bleu, form_score in zip(\n",
    "                informal_inputs[:10], references[:10], predictions[:10],\n",
    "                bleu_scores[:10], formality_improvements[:10]\n",
    "            )\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    # Save evaluation results\n",
    "    with open(os.path.join(base_output_dir, 'mistral_evaluation_results.json'), 'w') as f:\n",
    "        json.dump(eval_results, f, indent=2, default=str)\n",
    "    \n",
    "    return eval_results\n",
    "\n",
    "# Run evaluation\n",
    "print(\"🔍 Running Mistral formality translation evaluation...\")\n",
    "eval_results = evaluate_mistral_formality(test_size=20)\n",
    "\n",
    "print(f\"\\n✅ Evaluation complete on {device.upper()}! Results saved to mistral_evaluation_results.json\")\n",
    "\n",
    "# Memory cleanup\n",
    "cleanup_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dea328f",
   "metadata": {},
   "source": [
    "## 🚀 Mistral-Specific Optimizations and Insights\n",
    "\n",
    "### Key Adaptations for Mistral-7B-Instruct:\n",
    "\n",
    "1. **Instruction Format**: Used Mistral's native `<s>[INST] ... [/INST]` format for optimal performance\n",
    "2. **Memory Optimization**: 4-bit quantization with QLoRA enables training on T4 GPU (15GB VRAM)\n",
    "3. **Context Efficiency**: Reduced few-shot examples to 3 (vs 5 for GPT-2) to maximize context for actual task\n",
    "4. **Flash Attention**: Enabled for faster training and inference (GPU only)\n",
    "5. **Completion-Only Training**: Only fine-tune on the response portion, not the instruction\n",
    "6. **🆕 CPU Support**: Full CPU fallback with optimized settings for systems without CUDA\n",
    "\n",
    "### Performance Advantages:\n",
    "\n",
    "- **Better Instruction Following**: Mistral-7B-Instruct is pre-trained for instruction-following tasks\n",
    "- **Larger Context Window**: Better handling of longer text transformations\n",
    "- **More Consistent Output**: Instruction tuning reduces hallucinations and off-topic responses\n",
    "- **Professional Language**: Better understanding of formal/informal register differences\n",
    "- **🆕 Flexibility**: Can run on both GPU and CPU environments\n",
    "\n",
    "### T4 GPU Optimizations:\n",
    "\n",
    "- **Quantization**: 4-bit NF4 quantization reduces memory usage by ~75%\n",
    "- **Gradient Checkpointing**: Trades compute for memory efficiency\n",
    "- **Optimized Batch Size**: Batch size 2 with 8x gradient accumulation = effective batch size 16\n",
    "- **Mixed Precision**: BF16 for optimal T4 performance\n",
    "\n",
    "### 🆕 CPU Optimizations:\n",
    "\n",
    "- **No Quantization**: Full precision float32 for CPU compatibility\n",
    "- **Reduced LoRA Rank**: Lower rank (16 vs 64) to reduce memory usage\n",
    "- **Smaller Batch Sizes**: Batch size 1 with 4x gradient accumulation\n",
    "- **Conservative Generation**: Lower temperature and top_k for more stable CPU inference\n",
    "- **Shorter Sequences**: 512 tokens max vs 1024 for GPU\n",
    "\n",
    "### Next Steps for Production:\n",
    "\n",
    "1. **RLHF**: Apply reinforcement learning from human feedback for even better formality\n",
    "2. **Evaluation Expansion**: Add human evaluation metrics for formality assessment\n",
    "3. **Domain Adaptation**: Fine-tune for specific domains (legal, academic, business)\n",
    "4. **API Deployment**: Create REST API with proper input validation and rate limiting\n",
    "5. **🆕 Hybrid Deployment**: Use CPU for development/testing, GPU for production"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4627bb7a",
   "metadata": {},
   "source": [
    "## 📋 Comparison: GPT-2 vs Mistral-7B\n",
    "\n",
    "### Model Comparison Summary:\n",
    "\n",
    "| Aspect | GPT-2 Medium | Mistral-7B-Instruct |\n",
    "|--------|--------------|---------------------|\n",
    "| **Parameters** | 355M | 7B |\n",
    "| **Training Data** | General text | Instruction-tuned |\n",
    "| **Memory Usage** | ~2GB | ~4GB (4-bit) / ~28GB (CPU) |\n",
    "| **Context Length** | 1024 tokens | 4096+ tokens |\n",
    "| **Instruction Following** | Limited | Excellent |\n",
    "| **Formality Understanding** | Basic | Advanced |\n",
    "| **Fine-tuning Speed** | Fast | Moderate (GPU) / Slow (CPU) |\n",
    "| **Output Quality** | Good | Excellent |\n",
    "| **Hardware Requirements** | Low | Medium (T4+) / **🆕 CPU Compatible** |\n",
    "\n",
    "### When to Use Each:\n",
    "\n",
    "**Choose GPT-2 when:**\n",
    "- Limited compute resources\n",
    "- Fast prototyping needed\n",
    "- Simple formality transformations\n",
    "- Training data is abundant\n",
    "\n",
    "**Choose Mistral-7B when:**\n",
    "- High-quality output required\n",
    "- Complex instruction following needed\n",
    "- Professional deployment target\n",
    "- Modern GPU available (T4+) **🆕 or CPU for development**\n",
    "\n",
    "### 🆕 CPU vs GPU Trade-offs:\n",
    "\n",
    "**GPU Mode:**\n",
    "- ✅ Faster training (3 epochs)\n",
    "- ✅ Higher LoRA rank (better quality)\n",
    "- ✅ Larger batch sizes\n",
    "- ✅ 4-bit quantization (memory efficient)\n",
    "- ❌ Requires CUDA-enabled hardware\n",
    "\n",
    "**CPU Mode:**\n",
    "- ✅ No special hardware requirements\n",
    "- ✅ Good for development/testing\n",
    "- ✅ Functional training and inference\n",
    "- ❌ Slower training (1 epoch recommended)\n",
    "- ❌ Higher memory usage (no quantization)\n",
    "- ❌ Reduced batch sizes\n",
    "\n",
    "Both models demonstrate the effectiveness of few-shot learning for formality translation, with Mistral offering superior performance at the cost of increased computational requirements, **now with CPU fallback support for broader accessibility**."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
