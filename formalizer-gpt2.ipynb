{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Hearlvein/colab/blob/main/guten_tag.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "85e78fb2",
      "metadata": {
        "id": "85e78fb2"
      },
      "source": [
        "# üéØ Fine-Tuning GPT-2 for Formality Translation with Few-Shot Prompting\n",
        "\n",
        "This notebook guides you through fine-tuning GPT-2 to translate informal text to formal text using few-shot prompting. The process includes:\n",
        "- Dataset preparation from valentin_dataset.csv\n",
        "- Few-shot prompt engineering for formality translation\n",
        "- Model fine-tuning with LoRA\n",
        "- Interactive formality translation testing\n",
        "\n",
        "**Task:** Given an informal sentence, generate its formal equivalent using in-context learning.\n",
        "\n",
        "**Note:** This notebook is designed for execution in Google Colab."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "38cb754d",
      "metadata": {
        "id": "38cb754d"
      },
      "source": [
        "## üîß Setup and Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "4dc4a906",
      "metadata": {
        "id": "4dc4a906"
      },
      "outputs": [],
      "source": [
        "# Install necessary packages\n",
        "!pip install -q transformers datasets peft trl bitsandbytes accelerate\n",
        "!pip install -q pandas scikit-learn\n",
        "!pip install -q tf-keras"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "05b944d2",
      "metadata": {
        "id": "05b944d2"
      },
      "source": [
        "## üìö Dataset Preparation and Few-Shot Example Selection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "ae02ca8a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ae02ca8a",
        "outputId": "124b1b9b-4bcc-42ac-b1b0-85cce6dfa00b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset loaded with 2000 pairs\n",
            "Sample data:\n",
            "                                              formal  \\\n",
            "0  We kindly ask that you the system update will ...   \n",
            "1  Good morning, I regret the oversight and will ...   \n",
            "2  We kindly ask that you we have identified a di...   \n",
            "3  Esteemed colleagues, I regret the oversight an...   \n",
            "4  I would appreciate it if you could we require ...   \n",
            "\n",
            "                                            informal  \n",
            "0  We'd like you to we'll update the system this ...  \n",
            "1  Morning! My bad, I'll fix it ASAP. Mind sendin...  \n",
            "2  We'd like you to we found a mistake in the dat...  \n",
            "3  Hey folks, My bad, I'll fix it ASAP. Let me kn...  \n",
            "4  I'd be grateful if you we need more info to mo...  \n",
            "After cleaning: 2000 pairs\n",
            "\n",
            "Selected few-shot examples:\n",
            "\n",
            "1. Informal: Heads up that your doc needs more edits. All the best.\n",
            "   Formal: Kindly note that the document you provided requires further revision. Please accept my best regards.\n",
            "\n",
            "2. Informal: Heads up that your doc needs more edits. Thanks for understanding.\n",
            "   Formal: Kindly note that the document you provided requires further revision. Thank you for your understanding.\n",
            "\n",
            "3. Informal: Could you please your doc needs more edits. Thanks for sticking with us.\n",
            "   Formal: It would be appreciated if you could the document you provided requires further revision. We appreciate your continued support.\n",
            "\n",
            "4. Informal: Could you please your doc needs more edits. Really appreciate your help.\n",
            "   Formal: It would be appreciated if you could the document you provided requires further revision. Your cooperation is highly valued.\n",
            "\n",
            "5. Informal: Could you your doc needs more edits. Take care,\n",
            "   Formal: I would like to request that the document you provided requires further revision. Warmest regards,\n",
            "\n",
            "Selected few-shot examples:\n",
            "\n",
            "1. Informal: Heads up that your doc needs more edits. All the best.\n",
            "   Formal: Kindly note that the document you provided requires further revision. Please accept my best regards.\n",
            "\n",
            "2. Informal: Heads up that your doc needs more edits. Thanks for understanding.\n",
            "   Formal: Kindly note that the document you provided requires further revision. Thank you for your understanding.\n",
            "\n",
            "3. Informal: Could you please your doc needs more edits. Thanks for sticking with us.\n",
            "   Formal: It would be appreciated if you could the document you provided requires further revision. We appreciate your continued support.\n",
            "\n",
            "4. Informal: Could you please your doc needs more edits. Really appreciate your help.\n",
            "   Formal: It would be appreciated if you could the document you provided requires further revision. Your cooperation is highly valued.\n",
            "\n",
            "5. Informal: Could you your doc needs more edits. Take care,\n",
            "   Formal: I would like to request that the document you provided requires further revision. Warmest regards,\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "import random\n",
        "from pathlib import Path\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from typing import List, Tuple\n",
        "import re\n",
        "\n",
        "# Load the valentin dataset\n",
        "dataset_path = \"valentin_dataset.csv\"\n",
        "df = pd.read_csv(dataset_path, sep=';')\n",
        "\n",
        "print(f\"Dataset loaded with {len(df)} pairs\")\n",
        "print(\"Sample data:\")\n",
        "print(df.head())\n",
        "\n",
        "# Clean and validate the data\n",
        "def clean_text(text):\n",
        "    \"\"\"Clean text by removing extra whitespace and normalizing\"\"\"\n",
        "    if pd.isna(text):\n",
        "        return \"\"\n",
        "    return re.sub(r'\\s+', ' ', str(text).strip())\n",
        "\n",
        "df['formal'] = df['formal'].apply(clean_text)\n",
        "df['informal'] = df['informal'].apply(clean_text)\n",
        "\n",
        "# Remove empty or very short entries\n",
        "df = df[(df['formal'].str.len() > 10) & (df['informal'].str.len() > 10)]\n",
        "print(f\"After cleaning: {len(df)} pairs\")\n",
        "\n",
        "def select_diverse_examples(df: pd.DataFrame, n_examples: int = 5) -> List[Tuple[str, str]]:\n",
        "    \"\"\"\n",
        "    Select diverse examples for few-shot prompting using TF-IDF similarity\n",
        "    to ensure variety in the selected examples.\n",
        "    \"\"\"\n",
        "    # Use TF-IDF to find diverse examples\n",
        "    vectorizer = TfidfVectorizer(max_features=1000, stop_words='english')\n",
        "    informal_vectors = vectorizer.fit_transform(df['informal'])\n",
        "    \n",
        "    selected_indices = []\n",
        "    remaining_indices = list(range(len(df)))\n",
        "    \n",
        "    # Select first example randomly\n",
        "    first_idx = random.choice(remaining_indices)\n",
        "    selected_indices.append(first_idx)\n",
        "    remaining_indices.remove(first_idx)\n",
        "    \n",
        "    # Select remaining examples to maximize diversity\n",
        "    for _ in range(n_examples - 1):\n",
        "        if not remaining_indices:\n",
        "            break\n",
        "            \n",
        "        max_min_similarity = -1\n",
        "        best_idx = None\n",
        "        \n",
        "        for idx in remaining_indices:\n",
        "            # Calculate minimum similarity to already selected examples\n",
        "            similarities = []\n",
        "            for selected_idx in selected_indices:\n",
        "                sim = cosine_similarity(\n",
        "                    informal_vectors[idx:idx+1], \n",
        "                    informal_vectors[selected_idx:selected_idx+1]\n",
        "                )[0][0]\n",
        "                similarities.append(sim)\n",
        "            \n",
        "            min_similarity = min(similarities)\n",
        "            if min_similarity > max_min_similarity:\n",
        "                max_min_similarity = min_similarity\n",
        "                best_idx = idx\n",
        "        \n",
        "        if best_idx is not None:\n",
        "            selected_indices.append(best_idx)\n",
        "            remaining_indices.remove(best_idx)\n",
        "    \n",
        "    # Return selected examples\n",
        "    examples = []\n",
        "    for idx in selected_indices:\n",
        "        examples.append((df.iloc[idx]['informal'], df.iloc[idx]['formal']))\n",
        "    \n",
        "    return examples\n",
        "\n",
        "# Select diverse examples for few-shot prompting\n",
        "few_shot_examples = select_diverse_examples(df, n_examples=5)\n",
        "\n",
        "print(\"\\nSelected few-shot examples:\")\n",
        "for i, (informal, formal) in enumerate(few_shot_examples, 1):\n",
        "    print(f\"\\n{i}. Informal: {informal}\")\n",
        "    print(f\"   Formal: {formal}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cc1261d4",
      "metadata": {
        "id": "cc1261d4"
      },
      "source": [
        "## üéØ Few-Shot Prompt Engineering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "826ddea3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "826ddea3",
        "outputId": "0f6953de-8275-4808-a76c-76a6c4dfbc41"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Created 1995 training examples\n",
            "Training data saved to formality_dataset.jsonl\n",
            "\n",
            "Sample few-shot prompt:\n",
            "Task: Translate informal text to formal text while preserving the original meaning.\n",
            "\n",
            "Examples:\n",
            "\n",
            "Informal: Heads up that your doc needs more edits. All the best.\n",
            "Formal: Kindly note that the document you provided requires further revision. Please accept my best regards.\n",
            "\n",
            "Informal: Heads up that your doc needs more edits. Thanks for understanding.\n",
            "Formal: Kindly note that the document you provided requires further revision. Thank you for your understanding.\n",
            "\n",
            "Informal: Could you please your doc needs more edits. Thanks for sticking with us.\n",
            "Formal: It would be appreciated if you could the document you provided requires further revision. We appreciate your continued support.\n",
            "\n",
            "Informal: Could you please your doc needs more edits. Really appreciate your help.\n",
            "Formal: It would be appreciated if you could the document you provided requires further revision. Your cooperation is highly valued.\n",
            "\n",
            "Informal: Could you your doc needs more edits. Take care,\n",
            "Formal: I would like to request that the document you provided requires further revision. Warmest regards,\n",
            "\n",
            "Informal: Hey, can you help me out?\n",
            "Formal:\n"
          ]
        }
      ],
      "source": [
        "def create_formality_prompt(examples: List[Tuple[str, str]], test_informal: str = None) -> str:\n",
        "    \"\"\"\n",
        "    Create a few-shot prompt for formality translation.\n",
        "    \n",
        "    Args:\n",
        "        examples: List of (informal, formal) pairs for few-shot learning\n",
        "        test_informal: Optional informal sentence to translate\n",
        "    \n",
        "    Returns:\n",
        "        Formatted prompt string\n",
        "    \"\"\"\n",
        "    prompt = \"\"\"Task: Translate informal text to formal text while preserving the original meaning.\n",
        "\n",
        "Examples:\n",
        "\"\"\"\n",
        "    \n",
        "    for i, (informal, formal) in enumerate(examples, 1):\n",
        "        prompt += f\"\"\"\n",
        "Informal: {informal}\n",
        "Formal: {formal}\n",
        "\"\"\"\n",
        "    \n",
        "    if test_informal:\n",
        "        prompt += f\"\"\"\n",
        "Informal: {test_informal}\n",
        "Formal:\"\"\"\n",
        "    \n",
        "    return prompt\n",
        "\n",
        "def create_training_data_with_prompts(df: pd.DataFrame, few_shot_examples: List[Tuple[str, str]]) -> List[dict]:\n",
        "    \"\"\"\n",
        "    Create training data where each example includes few-shot context.\n",
        "    \"\"\"\n",
        "    training_data = []\n",
        "    \n",
        "    # Create a set of few-shot examples to exclude from training\n",
        "    few_shot_informals = {informal for informal, _ in few_shot_examples}\n",
        "    \n",
        "    for _, row in df.iterrows():\n",
        "        # Skip if this example is used in few-shot prompting\n",
        "        if row['informal'] in few_shot_informals:\n",
        "            continue\n",
        "            \n",
        "        # Create prompt with few-shot examples\n",
        "        prompt = create_formality_prompt(few_shot_examples, row['informal'])\n",
        "        full_text = prompt + \" \" + row['formal']\n",
        "        \n",
        "        training_data.append({\n",
        "            \"text\": full_text,\n",
        "            \"informal\": row['informal'],\n",
        "            \"formal\": row['formal']\n",
        "        })\n",
        "    \n",
        "    return training_data\n",
        "\n",
        "# Create training data with few-shot prompts\n",
        "training_data = create_training_data_with_prompts(df, few_shot_examples)\n",
        "print(f\"Created {len(training_data)} training examples\")\n",
        "\n",
        "# Save training data to JSONL\n",
        "output_file = Path(\"formality_dataset.jsonl\")\n",
        "with output_file.open(\"w\", encoding=\"utf-8\") as f:\n",
        "    for item in training_data:\n",
        "        f.write(json.dumps(item, ensure_ascii=False) + \"\\n\")\n",
        "\n",
        "print(f\"Training data saved to {output_file}\")\n",
        "\n",
        "# Show example prompt\n",
        "sample_prompt = create_formality_prompt(few_shot_examples, \"Hey, can you help me out?\")\n",
        "print(\"\\nSample few-shot prompt:\")\n",
        "print(sample_prompt)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f7a42369",
      "metadata": {
        "id": "f7a42369"
      },
      "source": [
        "## üß† Model Fine-Tuning with LoRA for Formality Translation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ceb3d2ce",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 465,
          "referenced_widgets": [
            "d4025865aee842d3a1800d6d73f5b967",
            "a77ad0df19044006bd28b8fb9a7c75a4",
            "1d7cd2e8f16b437092907e4e4ac5efff",
            "ab2f8044a2424aeb968ea8d413544b32",
            "43e891bb8a39471db48409ccce97ac8f",
            "3d88fa454c924aebbcdfdc74d409f372",
            "0ec92b52a92a45e782d98c7161222a02",
            "b0c31407646c43d8bc5b95fb766bbe76",
            "2293c5c237d74ef2b1ef74c2bbbc4769",
            "d5ff4d95965e4f8fb9a6afb72860aa3e",
            "e36569878fe94db08cdb8a3c59dbaa30",
            "6f8597d2e4cb43d9a30ff28ac8df03e6",
            "6377c17056a4427186f6748e7267bafa",
            "66ae97084fd9427ea5d32487a36a3926",
            "40f6bd5d16b24438a6da93eae747bfdb",
            "90a683bae74248678f42f6c7dcd14ea6",
            "e546d71c76bb4fec95b79aa8dcadb96d",
            "cf0ab7078ff94452ab29546a88c469b0",
            "270be6f139d14bf79bf4bac0d35e7126",
            "8207f2b8287d49b3920bb30631cacc50",
            "659b2f0f68d443b59c0b254b30c091d0",
            "b8574c2651fd4a3492e6730741105b78"
          ]
        },
        "id": "ceb3d2ce",
        "outputId": "5d9da6a6-8f62-4844-de44-9d7a4b45b319"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading formality translation dataset...\n",
            "Dataset loaded with 1995 records.\n",
            "Using device: cpu\n",
            "Quantization not available (Not using CUDA, falling back to regular loading), falling back to regular model loading...\n",
            "Using device: cpu\n",
            "Quantization not available (Not using CUDA, falling back to regular loading), falling back to regular model loading...\n",
            "Using regular model loading without quantization\n",
            "Using regular model loading without quantization\n"
          ]
        },
        {
          "ename": "TypeError",
          "evalue": "SFTTrainer.__init__() got an unexpected keyword argument 'tokenizer'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[9], line 86\u001b[0m\n\u001b[0;32m     83\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m: examples[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m]}\n\u001b[0;32m     85\u001b[0m \u001b[38;5;66;03m# Initialize trainer compatible with TRL 0.18.2\u001b[39;00m\n\u001b[1;32m---> 86\u001b[0m trainer \u001b[38;5;241m=\u001b[39m SFTTrainer(\n\u001b[0;32m     87\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m     88\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[0;32m     89\u001b[0m     train_dataset\u001b[38;5;241m=\u001b[39mtrain_dataset,\n\u001b[0;32m     90\u001b[0m     eval_dataset\u001b[38;5;241m=\u001b[39meval_dataset,\n\u001b[0;32m     91\u001b[0m     formatting_func\u001b[38;5;241m=\u001b[39mformatting_prompts_func,\n\u001b[0;32m     92\u001b[0m     tokenizer\u001b[38;5;241m=\u001b[39mtokenizer,\n\u001b[0;32m     93\u001b[0m     max_seq_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m512\u001b[39m,\n\u001b[0;32m     94\u001b[0m     packing\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,  \u001b[38;5;66;03m# Disable packing for cleaner training\u001b[39;00m\n\u001b[0;32m     95\u001b[0m )\n\u001b[0;32m     97\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m     98\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStarting formality translation training...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "\u001b[1;31mTypeError\u001b[0m: SFTTrainer.__init__() got an unexpected keyword argument 'tokenizer'"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from datasets import load_dataset, Dataset\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "from peft import get_peft_model, LoraConfig, prepare_model_for_kbit_training\n",
        "from trl import SFTTrainer, SFTConfig\n",
        "\n",
        "# Load the dataset\n",
        "print(\"Loading formality translation dataset...\")\n",
        "with open(output_file, 'r', encoding='utf-8') as f:\n",
        "    data = [json.loads(line) for line in f if line.strip()]\n",
        "dataset = Dataset.from_list(data)\n",
        "print(f\"Dataset loaded with {len(dataset)} records.\")\n",
        "\n",
        "# Load tokenizer and model\n",
        "MODEL_NAME = \"gpt2\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# Check device availability and configure model loading accordingly\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Try to use quantization if supported, otherwise fall back to regular loading\n",
        "try:\n",
        "    if device == \"cuda\":\n",
        "        # Load model with 8-bit precision for CUDA\n",
        "        bnb_config = BitsAndBytesConfig(load_in_8bit=True)\n",
        "        model = AutoModelForCausalLM.from_pretrained(\n",
        "            MODEL_NAME, \n",
        "            quantization_config=bnb_config, \n",
        "            device_map=\"auto\"\n",
        "        )\n",
        "        model = prepare_model_for_kbit_training(model)\n",
        "        print(\"Using 8-bit quantization with bitsandbytes\")\n",
        "    else:\n",
        "        raise RuntimeError(\"Not using CUDA, falling back to regular loading\")\n",
        "except Exception as e:\n",
        "    print(f\"Quantization not available ({e}), falling back to regular model loading...\")\n",
        "    # Fallback to regular model loading without quantization\n",
        "    model = AutoModelForCausalLM.from_pretrained(MODEL_NAME)\n",
        "    model = model.to(device)\n",
        "    print(\"Using regular model loading without quantization\")\n",
        "\n",
        "# Configure LoRA for formality translation\n",
        "lora_config = LoraConfig(\n",
        "    r=16,  # Slightly higher rank for better formality understanding\n",
        "    lora_alpha=32,\n",
        "    target_modules=[\"c_attn\", \"c_proj\"],\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\"\n",
        ")\n",
        "model = get_peft_model(model, lora_config)\n",
        "\n",
        "# Tokenization function\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples[\"text\"], truncation=True, padding=\"max_length\", max_length=512)\n",
        "\n",
        "# Tokenize the dataset\n",
        "tokenized_dataset = dataset.map(tokenize_function, batched=True, remove_columns=[\"text\", \"informal\", \"formal\"])\n",
        "\n",
        "# Training configuration optimized for formality translation\n",
        "batch_size = 2 if device == \"cpu\" else 4\n",
        "\n",
        "# Use SFTConfig compatible with TRL 0.18.2\n",
        "training_args = SFTConfig(\n",
        "    output_dir=\"./formality_translator_model\",\n",
        "    per_device_train_batch_size=batch_size,\n",
        "    num_train_epochs=3,\n",
        "    logging_steps=25,\n",
        "    save_strategy=\"epoch\",\n",
        "    fp16=(device == \"cuda\"),  # Only use fp16 with CUDA\n",
        "    push_to_hub=False,\n",
        "    report_to=\"none\",\n",
        "    overwrite_output_dir=True\n",
        ")\n",
        "\n",
        "# Initialize trainer compatible with TRL 0.18.2\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_dataset,\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "print(\"Starting formality translation training...\")\n",
        "trainer.train()\n",
        "\n",
        "# Save the model\n",
        "model_path = \"./formality_translator_model\"\n",
        "trainer.save_model(model_path)\n",
        "tokenizer.save_pretrained(model_path)\n",
        "print(f\"Formality translator model saved to {model_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1c02f4a9",
      "metadata": {
        "id": "1c02f4a9"
      },
      "source": [
        "## ‚ú® Formality Translation Testing and Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3ffb9c25",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ffb9c25",
        "outputId": "36a085c4-679d-431c-9097-4db35c5eb917"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "import random\n",
        "\n",
        "# Load the fine-tuned formality translator\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_path)\n",
        "generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
        "\n",
        "def translate_to_formal(informal_text: str, few_shot_examples: List[Tuple[str, str]]) -> str:\n",
        "    \"\"\"\n",
        "    Translate informal text to formal using few-shot prompting.\n",
        "    \"\"\"\n",
        "    prompt = create_formality_prompt(few_shot_examples, informal_text)\n",
        "    \n",
        "    output = generator(\n",
        "        prompt,\n",
        "        max_new_tokens=100,\n",
        "        do_sample=True,\n",
        "        temperature=0.7,\n",
        "        top_k=50,\n",
        "        top_p=0.9,\n",
        "        repetition_penalty=1.1,\n",
        "        num_return_sequences=1,\n",
        "        eos_token_id=tokenizer.eos_token_id,\n",
        "        pad_token_id=tokenizer.eos_token_id\n",
        "    )\n",
        "    \n",
        "    generated_text = output[0][\"generated_text\"]\n",
        "    # Extract only the formal translation (after the last \"Formal:\")\n",
        "    formal_part = generated_text.split(\"Formal:\")[-1].strip()\n",
        "    \n",
        "    # Clean up the output - take only the first sentence/phrase\n",
        "    formal_sentences = formal_part.split('\\n')[0].split('.')[0]\n",
        "    return formal_sentences.strip()\n",
        "\n",
        "# Test with some examples from the dataset\n",
        "test_examples = df.sample(5, random_state=42)\n",
        "\n",
        "print(\"üéØ Formality Translation Results:\\n\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "for idx, row in test_examples.iterrows():\n",
        "    informal_input = row['informal']\n",
        "    expected_formal = row['formal']\n",
        "    predicted_formal = translate_to_formal(informal_input, few_shot_examples)\n",
        "    \n",
        "    print(f\"\\nInput (Informal): {informal_input}\")\n",
        "    print(f\"Expected (Formal): {expected_formal}\")\n",
        "    print(f\"Generated (Formal): {predicted_formal}\")\n",
        "    print(\"-\" * 40)\n",
        "\n",
        "# Interactive testing function\n",
        "def interactive_formality_test():\n",
        "    \"\"\"\n",
        "    Interactive function to test formality translation with user input.\n",
        "    \"\"\"\n",
        "    print(\"\\nüîÑ Interactive Formality Translation Test\")\n",
        "    print(\"Enter informal sentences to see their formal translations.\")\n",
        "    print(\"Type 'quit' to exit.\\n\")\n",
        "    \n",
        "    while True:\n",
        "        user_input = input(\"Informal sentence: \").strip()\n",
        "        \n",
        "        if user_input.lower() in ['quit', 'exit', 'q']:\n",
        "            break\n",
        "            \n",
        "        if not user_input:\n",
        "            continue\n",
        "            \n",
        "        formal_output = translate_to_formal(user_input, few_shot_examples)\n",
        "        print(f\"Formal translation: {formal_output}\\n\")\n",
        "\n",
        "# Example translations\n",
        "example_informal_sentences = [\n",
        "    \"Hey, what's up?\",\n",
        "    \"Can you help me out with this thing?\",\n",
        "    \"Thanks a bunch for your help!\",\n",
        "    \"I'll get back to you ASAP.\",\n",
        "    \"Let me know if you need anything.\"\n",
        "]\n",
        "\n",
        "print(\"\\nüìù Example Translations:\")\n",
        "for informal in example_informal_sentences:\n",
        "    formal = translate_to_formal(informal, few_shot_examples)\n",
        "    print(f\"‚Ä¢ {informal} ‚Üí {formal}\")\n",
        "\n",
        "# Run interactive test (uncomment to use)\n",
        "# interactive_formality_test()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bf9b801f",
      "metadata": {},
      "source": [
        "## üìä Evaluation Metrics and Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9d859cb5",
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "import nltk\n",
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
        "from collections import Counter\n",
        "import re\n",
        "\n",
        "# Download required NLTK data\n",
        "try:\n",
        "    nltk.download('punkt', quiet=True)\n",
        "except:\n",
        "    pass\n",
        "\n",
        "def evaluate_formality_translation(test_size: int = 20):\n",
        "    \"\"\"\n",
        "    Evaluate the formality translation model using multiple metrics.\n",
        "    \"\"\"\n",
        "    # Select test examples (different from few-shot examples)\n",
        "    few_shot_informals = {informal for informal, _ in few_shot_examples}\n",
        "    test_df = df[~df['informal'].isin(few_shot_informals)].sample(test_size, random_state=42)\n",
        "    \n",
        "    predictions = []\n",
        "    references = []\n",
        "    \n",
        "    print(\"Evaluating formality translation...\")\n",
        "    \n",
        "    for _, row in test_df.iterrows():\n",
        "        informal_input = row['informal']\n",
        "        expected_formal = row['formal']\n",
        "        predicted_formal = translate_to_formal(informal_input, few_shot_examples)\n",
        "        \n",
        "        predictions.append(predicted_formal)\n",
        "        references.append(expected_formal)\n",
        "    \n",
        "    # Calculate BLEU scores\n",
        "    bleu_scores = []\n",
        "    smoothie = SmoothingFunction().method4\n",
        "    \n",
        "    for pred, ref in zip(predictions, references):\n",
        "        # Tokenize sentences\n",
        "        pred_tokens = pred.lower().split()\n",
        "        ref_tokens = ref.lower().split()\n",
        "        \n",
        "        # Calculate BLEU score\n",
        "        bleu = sentence_bleu([ref_tokens], pred_tokens, smoothing_function=smoothie)\n",
        "        bleu_scores.append(bleu)\n",
        "    \n",
        "    avg_bleu = np.mean(bleu_scores)\n",
        "    \n",
        "    # Analyze formality indicators\n",
        "    formal_indicators = [\n",
        "        'please', 'kindly', 'would', 'could', 'sincerely', 'respectfully',\n",
        "        'appreciate', 'grateful', 'thank you', 'regards', 'esteemed'\n",
        "    ]\n",
        "    \n",
        "    informal_indicators = [\n",
        "        'hey', 'hi', 'thanks', 'gonna', 'wanna', 'yeah', 'ok', 'asap'\n",
        "    ]\n",
        "    \n",
        "    def count_indicators(text, indicators):\n",
        "        text_lower = text.lower()\n",
        "        return sum(1 for indicator in indicators if indicator in text_lower)\n",
        "    \n",
        "    formal_gains = []\n",
        "    informal_reductions = []\n",
        "    \n",
        "    for pred, informal in zip(predictions, test_df['informal']):\n",
        "        # Count formal indicators gained\n",
        "        formal_gain = count_indicators(pred, formal_indicators) - count_indicators(informal, formal_indicators)\n",
        "        formal_gains.append(max(0, formal_gain))\n",
        "        \n",
        "        # Count informal indicators reduced\n",
        "        informal_reduction = count_indicators(informal, informal_indicators) - count_indicators(pred, informal_indicators)\n",
        "        informal_reductions.append(max(0, informal_reduction))\n",
        "    \n",
        "    print(f\"\\nüìä Evaluation Results (n={test_size}):\")\n",
        "    print(\"=\"*50)\n",
        "    print(f\"Average BLEU Score: {avg_bleu:.3f}\")\n",
        "    print(f\"Average Formal Indicators Added: {np.mean(formal_gains):.2f}\")\n",
        "    print(f\"Average Informal Indicators Removed: {np.mean(informal_reductions):.2f}\")\n",
        "    \n",
        "    # Show some example results\n",
        "    print(f\"\\nüìù Sample Results:\")\n",
        "    for i in range(min(3, len(predictions))):\n",
        "        print(f\"\\nExample {i+1}:\")\n",
        "        print(f\"Informal: {test_df.iloc[i]['informal']}\")\n",
        "        print(f\"Reference: {test_df.iloc[i]['formal']}\")\n",
        "        print(f\"Generated: {predictions[i]}\")\n",
        "        print(f\"BLEU: {bleu_scores[i]:.3f}\")\n",
        "    \n",
        "    return {\n",
        "        'bleu_scores': bleu_scores,\n",
        "        'avg_bleu': avg_bleu,\n",
        "        'formal_gains': formal_gains,\n",
        "        'informal_reductions': informal_reductions,\n",
        "        'predictions': predictions,\n",
        "        'references': references\n",
        "    }\n",
        "\n",
        "# Run evaluation\n",
        "evaluation_results = evaluate_formality_translation(test_size=15)\n",
        "\n",
        "print(f\"\\nüéØ Model Performance Summary:\")\n",
        "print(f\"The formality translator achieves an average BLEU score of {evaluation_results['avg_bleu']:.3f}\")\n",
        "print(f\"Successfully adds formal language indicators and reduces informal ones.\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "include_colab_link": true,
      "provenance": []
    },
    "jupytext": {
      "cell_metadata_filter": "-all",
      "main_language": "python",
      "notebook_metadata_filter": "-all"
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0ec92b52a92a45e782d98c7161222a02": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1d7cd2e8f16b437092907e4e4ac5efff": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b0c31407646c43d8bc5b95fb766bbe76",
            "max": 20,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2293c5c237d74ef2b1ef74c2bbbc4769",
            "value": 20
          }
        },
        "2293c5c237d74ef2b1ef74c2bbbc4769": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "270be6f139d14bf79bf4bac0d35e7126": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3d88fa454c924aebbcdfdc74d409f372": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "40f6bd5d16b24438a6da93eae747bfdb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_659b2f0f68d443b59c0b254b30c091d0",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_b8574c2651fd4a3492e6730741105b78",
            "value": "‚Äá20/20‚Äá[00:00&lt;00:00,‚Äá649.99‚Äáexamples/s]"
          }
        },
        "43e891bb8a39471db48409ccce97ac8f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6377c17056a4427186f6748e7267bafa": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e546d71c76bb4fec95b79aa8dcadb96d",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_cf0ab7078ff94452ab29546a88c469b0",
            "value": "Truncating‚Äátrain‚Äádataset:‚Äá100%"
          }
        },
        "659b2f0f68d443b59c0b254b30c091d0": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "66ae97084fd9427ea5d32487a36a3926": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_270be6f139d14bf79bf4bac0d35e7126",
            "max": 20,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8207f2b8287d49b3920bb30631cacc50",
            "value": 20
          }
        },
        "6f8597d2e4cb43d9a30ff28ac8df03e6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6377c17056a4427186f6748e7267bafa",
              "IPY_MODEL_66ae97084fd9427ea5d32487a36a3926",
              "IPY_MODEL_40f6bd5d16b24438a6da93eae747bfdb"
            ],
            "layout": "IPY_MODEL_90a683bae74248678f42f6c7dcd14ea6"
          }
        },
        "8207f2b8287d49b3920bb30631cacc50": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "90a683bae74248678f42f6c7dcd14ea6": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a77ad0df19044006bd28b8fb9a7c75a4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3d88fa454c924aebbcdfdc74d409f372",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_0ec92b52a92a45e782d98c7161222a02",
            "value": "Map:‚Äá100%"
          }
        },
        "ab2f8044a2424aeb968ea8d413544b32": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d5ff4d95965e4f8fb9a6afb72860aa3e",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_e36569878fe94db08cdb8a3c59dbaa30",
            "value": "‚Äá20/20‚Äá[00:27&lt;00:00,‚Äá‚Äá1.38s/‚Äáexamples]"
          }
        },
        "b0c31407646c43d8bc5b95fb766bbe76": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b8574c2651fd4a3492e6730741105b78": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cf0ab7078ff94452ab29546a88c469b0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d4025865aee842d3a1800d6d73f5b967": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a77ad0df19044006bd28b8fb9a7c75a4",
              "IPY_MODEL_1d7cd2e8f16b437092907e4e4ac5efff",
              "IPY_MODEL_ab2f8044a2424aeb968ea8d413544b32"
            ],
            "layout": "IPY_MODEL_43e891bb8a39471db48409ccce97ac8f"
          }
        },
        "d5ff4d95965e4f8fb9a6afb72860aa3e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e36569878fe94db08cdb8a3c59dbaa30": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e546d71c76bb4fec95b79aa8dcadb96d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
